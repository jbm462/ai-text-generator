# Mini-GPT Learning Roadmap

## What We've Built So Far
- Basic character-level text generation
- Pattern recognition in text
- Simple probability-based generation

## Phase 1: Improved Character Model
- N-gram modeling (looking at groups of characters)
- Basic grammar rules
- Better text generation
- **Learning Goals**: 
  - Understanding sequences
  - Probability distributions
  - Basic linguistic patterns

## Phase 2: Word-Level Model
- Moving from characters to words
- Word relationships
- Basic sentence structure
- **Learning Goals**:
  - Tokenization
  - Word patterns
  - Basic NLP concepts

## Phase 3: Neural Network Basics
- Simple neural network architecture
- Word embeddings
  - How AI represents words as numbers
  - Why some words are "closer" to others
- Attention mechanisms
  - How AI focuses on important parts
  - Why context matters
- Training loops
  - How AI learns from examples
  - How models improve over time

## Future Extensions
- Fine-tuning pre-trained models
- Working with larger datasets
- Understanding transformers architecture

## Resources
- Karpathy's minGPT tutorial
- Python basics needed for each phase
- Key machine learning concepts

## Best Practices
- Comment code for learning
- Build incrementally
- Test each component
- Understand before moving on